{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the environment variables for HuggingFace\n",
        "# This is done to ensure that the cache directory for HuggingFace is set to a specific location,\n",
        "# preventing the storage from being overwhelmed with model files and other data.\n",
        "SCRATCH = Path.home() / \"scratch\"\n",
        "os.environ[\"HF_HOME\"] = str(SCRATCH / \"hf_home\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/htkumar/torchtune/deep_rl/nano_aha_moment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1199204965232842,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import re\n",
        "import time\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "\n",
        "import deepspeed\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from deepspeed import DeepSpeedEngine\n",
        "from tqdm import trange\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# TODO: Add deepspeed params if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
        "MODEL_CHAT_NAME = MODEL_NAME + \"-Instruct\"\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
        "\n",
        "NUM_ITERATIONS = 1000\n",
        "EPISODES_PER_ITERATION = 64\n",
        "GENERATIONS_PER_SAMPLE = 4\n",
        "KL_COEFFICIENT = 0.001\n",
        "\n",
        "# actual batch size is 64, this is mbs so we are using grad_acc\n",
        "PER_DEVICE_BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-6\n",
        "\n",
        "# Sampling params\n",
        "MAX_RESPONSE_TOKENS = 1024\n",
        "TEMPERATURE = 1.0\n",
        "TOP_P = 1.0 # disabled nuclear sampling\n",
        "TOP_K = -1 # no top_k\n",
        "\n",
        "# TODO: define deepspeed configs here if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_NAME = \"r1-zero\"\n",
        "EXP_DIR = SCRATCH / \"deepseek_r1_replica\" / RUN_NAME\n",
        "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXP_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prompt_utils import (\n",
        "    DEFAULT_SYSTEM_MESSAGE,\n",
        "    DEFAULT_PROMPT_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We use the chat model tokenizer so that we can use `apply_chat_template` to the prompt\n",
        "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(MODEL_CHAT_NAME)\n",
        "EOS_TOKEN_ID = AutoTokenizer.from_pretrained(MODEL_NAME).eos_token_id\n",
        "EOS_TOKEN = tokenizer.convert_ids_to_tokens(EOS_TOKEN_ID)\n",
        "EOS_TOKEN_ID, EOS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_countdown_example(example: Dict[str, Any]):\n",
        "    numbers: List[int] = example[\"nums\"]\n",
        "    target: int = example[\"target\"]\n",
        "    prompt = DEFAULT_PROMPT_TEMPLATE.format(numbers=numbers, target=target)\n",
        "\n",
        "    chat_messages = [\n",
        "        {\"role\": \"system\", \"content\": DEFAULT_SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": \"Let me think step by step\\n<think>\"},\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        chat_messages, tokenize=True, continue_final_message=True\n",
        "    )\n",
        "    prompt = tokenizer.decode(\n",
        "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"prompt\": prompt,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_NAME, split='train')\n",
        "dataset = dataset.map(preprocess_countdown_example, num_proc=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[0]['prompt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_test_split = dataset.train_test_split(test_size=500, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n",
        "len(train_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset[0]['nums']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset[0]['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EOS_TOKEN = \"<|endoftext|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_reward_func(completion: str) -> float:\n",
        "    \"\"\"\n",
        "    Format: <think>...</think>\\n<answer>...</answer>\n",
        "    \"\"\"\n",
        "    allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
        "    try:\n",
        "        completion = \"<think>\" + completion\n",
        "\n",
        "        if completion.endswith(EOS_TOKEN):\n",
        "            completion = completion[:-len(EOS_TOKEN)]\n",
        "\n",
        "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
        "        match = re.search(regex, completion, re.DOTALL)\n",
        "        if match is None or len(match.groups()) != 2:\n",
        "            return 0.0\n",
        "        else:\n",
        "            answer_content = match.group(2).strip()\n",
        "            if not re.match(allowed_pattern, answer_content):\n",
        "                return 0.5\n",
        "            else:\n",
        "                return 1.0\n",
        "\n",
        "    except Exception:\n",
        "        return 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 874173324877555,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 1401432034229140,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "format_reward_func(\n",
        "    \"\"\"Using the numbers [4, 3, 56, 41], create an equation that equals 97.</think>\n",
        "First, I'll add 41 to 48 (56 + 4 - 6) to get 48. Then, I'll subtract 3 (since it's leftover) to get 45. Now, I have 48 and 45, which add up to 93. So, I need another 7 to get 97. I know that 7 is 14/2, so I'll multiply 48 by 14 which equals 672, then divide it by 2 (i.e. 672 / 2). Finally, I'll subtract it from 97 to achieve that difference of 7. Therefore, the final equation is <answer>96 - (48 * (672 / 2)) = 97</answer>.â‹…<|endoftext|>\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 678638591744468,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "format_reward_func(\"I am thinking </think>\\n<answer>abcd</answer>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "format_reward_func(\"I am thinking </think>\\n<answer>1+2</answer>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "format_reward_func(\"I am <thinking> </think>\\n<answer>1+2</answer>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "format_reward_func(\"I am <think> </think>\\n<answer>1+2</answer>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def equation_reward_func(completion: str, nums: List[int], target: int) -> float:\n",
        "    try:\n",
        "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
        "        if match is None:\n",
        "            return 0.0\n",
        "\n",
        "        equation = match.group(1).strip()\n",
        "        used_numbers = [int(n) for n in re.findall(r\"\\d+\", equation)]\n",
        "\n",
        "        if sorted(used_numbers) != sorted(nums):\n",
        "            return 0.0\n",
        "\n",
        "        allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
        "        if not re.match(allowed_pattern, equation):\n",
        "            return 0.0\n",
        "\n",
        "        result = eval(equation, {\"__builtins__\": None}, {})\n",
        "        if abs(float(result) - float(target)) < 1e-6:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    except Exception:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_reward(completion: str, sample: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
        "    format_reward = format_reward_func(completion)\n",
        "    equation_reward = equation_reward_func(completion, sample['nums'], sample['target'])\n",
        "    # todo: make this weighted?\n",
        "    reward = 1.0 * format_reward + 1.0 * equation_reward\n",
        "    metrics = {\n",
        "        'format_reward': format_reward,\n",
        "        \"equation_reward\": equation_reward,\n",
        "    }\n",
        "    return reward, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from reward_functions import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "equation_reward_func(\"I am thinking </think><answer>1+2</answer>\", [1, 2], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "equation_reward_func(\"I am thinking </think><answer>1+2+2</answer>\", [1, 2], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "equation_reward_func(\"I am thinking </think><answer>1+4</answer>\", [1, 2], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "samples = [{\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6}]\n",
        "all_generations = [[4,5, EOS_TOKEN_ID], [6,7], [8,9, EOS_TOKEN_ID], [10, 11]]  # 3 generations per sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "groups = [\n",
        "        list(range(i, i + GENERATIONS_PER_SAMPLE))\n",
        "        for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_query_token_ids = [\n",
        "        [sample[\"input_ids\"]] * GENERATIONS_PER_SAMPLE for sample in samples\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generation_strs = tokenizer.batch_decode(\n",
        "        all_generations, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generation_strs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generations_str_grouped = [[generation_strs[i] for i in group] for group in groups]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generations_str_grouped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards = [\n",
        "        [compute_reward(generation_str, sample) for generation_str in generations]\n",
        "        for sample, generations in zip(samples, generations_str_grouped)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards = [\n",
        "        [compute_reward(generation_str, sample)[0] for generation_str in generations]\n",
        "        for sample, generations in zip(samples, generations_str_grouped)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards = np.array(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards.mean(), rewards.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = np.array(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a.mean(), a.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "arr = [\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 6, 7, 8]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b = np.array(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import create_training_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "case_0 = {\n",
        "    \"samples\": [{\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6}],\n",
        "    \"all_generations\": [[4,5, 22, 33], [6,7], [8,9, 11], [10,11]],\n",
        "    \"all_finish_reasons\": [\"stop\", \"length\", \"stop\", \"stop\"]\n",
        "}\n",
        "create_training_episodes(tokenizer, **case_0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "case_1 = {\n",
        "    \"samples\": [{\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6}, {\"input_ids\": [9, 8, 7, 6, 5, 4], \"nums\": [1,2,3,4], \"target\": 10}],\n",
        "    \"all_generations\": [[4,5, 22, 33], [6,7], [8,9, 11], [10,11], [9,10], [11,12], [13,14], [15,16]],\n",
        "    \"all_finish_reasons\": [\"stop\", \"length\", \"stop\", \"stop\", \"length\", \"length\", \"stop\", \"stop\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "episodes, stats = create_training_episodes(tokenizer, **case_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(create_training_episodes(tokenizer, **case_1)[0]['all_advantages'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import prepare_model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_model_inputs1(\n",
        "    training_episodes: Dict[str, Any], device: torch.device\n",
        ") -> Dict[str, torch.tensor]:\n",
        "    query_token_ids = training_episodes[\"all_query_token_ids\"]\n",
        "    response_token_ids = training_episodes[\"all_response_token_ids\"]\n",
        "    advantages = training_episodes[\"all_advantages\"]\n",
        "    print(len(query_token_ids), len(response_token_ids), len(advantages))\n",
        "\n",
        "    max_seq_len = max(\n",
        "        len(q) + len(r) for q, r in zip(query_token_ids, response_token_ids)\n",
        "    )\n",
        "    input_ids, attention_mask, labels, advantage_list = [], [], [], []\n",
        "    pad_token_id = 0\n",
        "    ignore_index = -100  # check nn.CrossEntropyLoss for more context\n",
        "\n",
        "    for q, r, a in zip(query_token_ids, response_token_ids, advantages):\n",
        "        # print(q)\n",
        "        combined_ids = q + r\n",
        "        seq_len = len(combined_ids)\n",
        "        # print(f\"seq_len is {len(seq_len)}\")\n",
        "        input_ids.append(combined_ids + [pad_token_id] * (max_seq_len - seq_len))\n",
        "        attention_mask.append([1] * seq_len + [0] * (max_seq_len - seq_len))\n",
        "        labels.append(\n",
        "            [ignore_index] * len(q) + r + [ignore_index] * (max_seq_len - seq_len)\n",
        "        )\n",
        "        advantage_list.append([0.0] * len(q) + a + [0.0] * (max_seq_len - seq_len))\n",
        "\n",
        "    print(len(input_ids))\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long, device=device),\n",
        "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long, device=device),\n",
        "        \"labels\": torch.tensor(labels, dtype=torch.long, device=device),\n",
        "        \"advantages\": torch.tensor(advantage_list, dtype=torch.float, device=device),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prepare_model_inputs(\n",
        "    episodes, \"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logits = torch.ones(4, 4, 8)\n",
        "logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.softmax(logits, dim=-1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.log_softmax(logits, dim=-1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.log(torch.softmax(logits, dim=-1))[0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = torch.tensor([0.1, 0.5, 0.9])\n",
        "torch.log(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_token_log_probs(\n",
        "    model: PreTrainedModel,\n",
        "    inputs: Dict[str, torch.tensor],\n",
        "    temperature: float,\n",
        ") -> torch.tensor:\n",
        "    \"\"\"\n",
        "    Compute the log probabilities of the next token given the input sequence.\n",
        "    \"\"\"\n",
        "    model_output = model(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        return_dict=True,\n",
        "        use_cache=False,\n",
        "    )\n",
        "    logits = model_output.logits.float() / temperature\n",
        "    shift_logits = logits[..., :-1, :].contiguous()  # [bsz, seq_len-1, vocab_size]\n",
        "    shift_labels = inputs[\"labels\"][..., 1:].contiguous()  # [bsz, seq_len-1]\n",
        "\n",
        "    label_mask = (shift_labels != -100).float()\n",
        "    shift_labels[shift_labels == -100] = 0\n",
        "\n",
        "    # [bsz, seq_len-1, vocab_size]\n",
        "    log_probs = torch.log_softmax(shift_logits, dim=-1)  # log(softmax(logits))\n",
        "\n",
        "    # [bsz, seq_len-1]\n",
        "    log_probs = torch.gather(\n",
        "        log_probs, dim=-1, index=shift_labels.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "    # [bsz, seq_len-1]\n",
        "    log_probs = log_probs * label_mask\n",
        "    return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_pg_loss(\n",
        "    policy_model: PreTrainedModel,\n",
        "    reference_model: PreTrainedModel,\n",
        "    input_batch: Dict[str, torch.tensor],\n",
        "    total_response_len: int,\n",
        ") -> Tuple[torch.tensor, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Compute the policy gradient loss for the policy model by combining PPO loss and KL penalty.\n",
        "    \"\"\"\n",
        "    # inputs are dim [bsz, seq_len]\n",
        "\n",
        "    # [bsz, seq_len-1]\n",
        "    with torch.no_grad():\n",
        "        ref_model_logprobs = compute_token_log_probs(\n",
        "            reference_model, input_batch, TEMPERATURE\n",
        "        )\n",
        "\n",
        "    policy_model_logprobs = compute_token_log_probs(\n",
        "        policy_model, input_batch, TEMPERATURE\n",
        "    )\n",
        "    diff = ref_model_logprobs - policy_model_logprobs\n",
        "    kl_distance = torch.exp(diff) - diff - 1\n",
        "    policy_loss = (\n",
        "        -policy_model_logprobs * input_batch[\"advantages\"][..., 1:]\n",
        "    )  # [bsz, seq_len-1]\n",
        "    loss = (\n",
        "        policy_loss + KL_COEFFICIENT * kl_distance\n",
        "    ).sum() / total_response_len  # scalar\n",
        "\n",
        "    metrics = {\n",
        "        \"policy_loss\": policy_loss.sum().item() / total_response_len,\n",
        "        \"kl_distance\": kl_distance.sum().item() / total_response_len,\n",
        "        # entropy should decrease over time as the policy becomes more certain, for reference model it should stay the same over time.\n",
        "        \"entropy_policy\": -policy_model_logprobs.sum().item() / total_response_len,\n",
        "        \"entropy_ref\": -ref_model_logprobs.sum().item() / total_response_len,\n",
        "    }\n",
        "    return loss, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=0,\n",
        ")\n",
        "reference_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_engine = LLM(\n",
        "    model=MODEL_NAME,\n",
        "    skip_tokenizer_init=False,\n",
        "    enable_prefix_caching=True,\n",
        "    swap_space=1,\n",
        "    scheduling_policy='fcfs',\n",
        "    dtype=torch.bfloat16,\n",
        "    max_model_len=2048,\n",
        "    enable_sleep_mode=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "671933ae-ede0-49de-b00c-9da8b0592639",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "deep_rl (local)",
      "language": "python",
      "name": "deep_rl_local"
    }
  }
}
