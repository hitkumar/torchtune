{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set the environment variables for HuggingFace\n",
        "# This is done to ensure that the cache directory for HuggingFace is set to a specific location,\n",
        "# preventing the storage from being overwhelmed with model files and other data.\n",
        "SCRATCH = Path.home() / \"scratch\"\n",
        "os.environ[\"HF_HOME\"] = str(SCRATCH / \"hf_home\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home/htkumar/torchtune/deep_rl/nano_aha_moment\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import re\n",
        "import time\n",
        "from typing import Any, Dict, List, Tuple, Union\n",
        "\n",
        "import deepspeed\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from deepspeed import DeepSpeedEngine\n",
        "from tqdm import trange\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# TODO: Add deepspeed params if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
        "MODEL_CHAT_NAME = MODEL_NAME + \"-Instruct\"\n",
        "\n",
        "# Dataset configuration\n",
        "DATASET_NAME = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
        "\n",
        "NUM_ITERATIONS = 1000\n",
        "EPISODES_PER_ITERATION = 64\n",
        "GENERATIONS_PER_SAMPLE = 4\n",
        "KL_COEFFICIENT = 0.001\n",
        "\n",
        "# actual batch size is 64, this is mbs so we are using grad_acc\n",
        "PER_DEVICE_BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-6\n",
        "\n",
        "# Sampling params\n",
        "MAX_RESPONSE_TOKENS = 1024\n",
        "TEMPERATURE = 1.0\n",
        "TOP_P = 1.0 # disabled nuclear sampling\n",
        "TOP_K = -1 # no top_k\n",
        "\n",
        "# TODO: define deepspeed configs here if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_NAME = \"r1-zero\"\n",
        "EXP_DIR = SCRATCH / \"deepseek_r1_replica\" / RUN_NAME\n",
        "EXP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EXP_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from prompt_utils import (\n",
        "    SYSTEM_MESSAGE,\n",
        "    PROMPT_TEMPLATE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We use the chat model tokenizer so that we can use `apply_chat_template` to the prompt\n",
        "tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(MODEL_CHAT_NAME)\n",
        "EOS_TOKEN_ID = AutoTokenizer.from_pretrained(MODEL_NAME).eos_token_id\n",
        "EOS_TOKEN = tokenizer.convert_ids_to_tokens(EOS_TOKEN_ID)\n",
        "EOS_TOKEN_ID, EOS_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_countdown_example(example: Dict[str, Any]):\n",
        "    numbers: List[int] = example[\"nums\"]\n",
        "    target: int = example[\"target\"]\n",
        "    prompt = PROMPT_TEMPLATE.format(numbers=numbers, target=target)\n",
        "\n",
        "    chat_messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": \"Let me think step by step\\n<think>\"},\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        chat_messages, tokenize=True, continue_final_message=True\n",
        "    )\n",
        "    prompt = tokenizer.decode(\n",
        "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"prompt\": prompt,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_NAME, split='train')\n",
        "dataset = dataset.map(preprocess_countdown_example, num_proc=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset[0]['prompt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_test_split = dataset.train_test_split(test_size=500, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "test_dataset = train_test_split['test']\n",
        "len(train_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset[0]['nums']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset[0]['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "671933ae-ede0-49de-b00c-9da8b0592639",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "deep_rl (local)",
      "language": "python",
      "name": "deep_rl_local"
    }
  }
}
